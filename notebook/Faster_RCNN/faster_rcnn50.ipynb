{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc67b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edoardo\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Edoardo\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 1/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:11<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 49.8442\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:06<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 8.8469\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.3200683891773224\n",
      "\n",
      "\n",
      "Epoca 2/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 35.1666\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 7.0129\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.3753499984741211\n",
      "\n",
      "\n",
      "Epoca 3/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 23.1115\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 5.9534\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.40663155913352966\n",
      "\n",
      "\n",
      "Epoca 4/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 16.2887\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.0756\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:06<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.4163799583911896\n",
      "\n",
      "\n",
      "Epoca 5/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:11<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 14.2460\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.2530\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.41756150126457214\n",
      "\n",
      "\n",
      "Epoca 6/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 13.6011\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.7496\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.40954074263572693\n",
      "\n",
      "\n",
      "Epoca 7/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 12.7985\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.5547\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.4075435698032379\n",
      "\n",
      "\n",
      "Epoca 8/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 12.7087\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.6675\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.40744587779045105\n",
      "\n",
      "\n",
      "Epoca 9/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 12.5950\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.6241\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.40746641159057617\n",
      "\n",
      "\n",
      "Epoca 10/10: \n",
      "\tLoss sul train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 130/130 [00:10<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 12.4641\n",
      "\tLoss sul Valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t: 100%|██████████| 25/25 [00:05<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t-Score: 6.6412\n",
      "\tEvaluation sul valid set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 25/25 [00:05<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.40723878145217896\n",
      "\n",
      "\n",
      "Training complete. Model saved.\n",
      "Score sul test set:\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEvaluating: 100%|██████████| 15/15 [00:00<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tmap_50: 0.42436742782592773\n",
      "saved prediction to: prova5/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edoardo\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torchvision\\utils.py:211: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root, image_set=\"train\", transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        image_set_file = os.path.join(root, \"ImageSets\", \"Main\", f\"{image_set}.txt\")\n",
    "        with open(image_set_file) as f:\n",
    "            self.image_ids = [x.strip() for x in f.readlines()]\n",
    "        self.img_dir = os.path.join(root, \"JPEGImages\")\n",
    "        self.ann_dir = os.path.join(root, \"Annotations\")\n",
    "        self.classes = [\"__background__\"] + self._find_classes()\n",
    "        self.image_files = sorted([f for f in os.listdir(self.img_dir) if f.endswith((\".jpg\", \".png\"))])\n",
    "\n",
    "    def _find_classes(self):\n",
    "        cls = set()\n",
    "        for fname in os.listdir(self.ann_dir):\n",
    "            tree = ET.parse(os.path.join(self.ann_dir, fname))\n",
    "            for obj in tree.findall(\"object\"):\n",
    "                cls.add(obj.find(\"name\").text)\n",
    "        return sorted(list(cls))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{image_id}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{image_id}.xml\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        tree = ET.parse(ann_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            label = obj.find(\"name\").text\n",
    "            labels.append(self.classes.index(label))\n",
    "\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            bbox = [\n",
    "                float(bndbox.find(\"xmin\").text),\n",
    "                float(bndbox.find(\"ymin\").text),\n",
    "                float(bndbox.find(\"xmax\").text),\n",
    "                float(bndbox.find(\"ymax\").text),\n",
    "            ]\n",
    "            boxes.append(bbox)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            # 🔁 Prova a caricare un'altra immagine se questa è vuota\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)   \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "def evaluate_metrics(experiment_name,model, data_loader, device,epoch,set):\n",
    "    csv_path = f\"{experiment_name}/{set}_metrics.csv\"\n",
    "    mAP50 = 0.0\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"\\t\\tEvaluating\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            metric.update(outputs, targets)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    results = metric.compute()\n",
    "\n",
    "    # Print\n",
    "    \n",
    "    for k, v in results.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            if(k==\"map_50\"):\n",
    "                print(f\"\\t\\t{k}: {v.item() if v.numel() == 1 else v}\")\n",
    "                mAP50 = v.item()\n",
    "    metric.reset()\n",
    "    # Save to CSV\n",
    "    summary = {k: v.item() if isinstance(v, torch.Tensor) and v.numel() == 1 else str(v)\n",
    "               for k, v in results.items()}\n",
    "    #summary[\"epoch\"] = epoch\n",
    "    summary = dict([(\"epoch\", epoch)] + list(summary.items()))\n",
    "    df = pd.DataFrame([summary])\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "    df.to_csv(csv_path, mode='a', header=write_header, index=False)\n",
    "    #print(f\"\\nSaved metrics to: {csv_path}\")\n",
    "    return mAP50\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def show_predictions(experiment_name,model, dataset, device, score_threshold=0.25,class_names=[]):\n",
    "    model.eval()\n",
    "    output_dir=f\"{experiment_name}/predictions\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        img, target = dataset[idx]\n",
    "        img_tensor = img.to(device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[0]\n",
    "\n",
    "        # Filtro predizioni con punteggio basso\n",
    "        keep = output['scores'] >= score_threshold\n",
    "        pred_boxes = output['boxes'][keep].cpu()\n",
    "        pred_labels = output['labels'][keep].cpu()\n",
    "        scores = output['scores'][keep].cpu()\n",
    "\n",
    "        # Disegna box predetti\n",
    "        pred_img = draw_bounding_boxes((img * 255).byte(), pred_boxes, labels=[f\"{class_names[l.item()]}: {s.item():.2f}\" for l,s in zip(pred_labels,scores)],\n",
    "                                       colors=\"green\", width=2,font_size=20,font=\"arial\")\n",
    "\n",
    "        # Disegna box reali\n",
    "        # true_boxes = target['boxes'].cpu()\n",
    "        # true_labels = target['labels'].cpu()\n",
    "        # full_img = draw_bounding_boxes(pred_img, true_boxes, labels=[f\"T:{l.item()}\" for l in true_labels],\n",
    "        #                                colors=\"green\", width=2)\n",
    "\n",
    "        \n",
    "\n",
    "        # Nome originale immagine\n",
    "        original_filename = dataset.image_files[idx] # Es: \"img001.jpg\"\n",
    "        output_filename = os.path.splitext(original_filename)[0] + \"_pred.jpg\"\n",
    "\n",
    "        # Salva immagine\n",
    "        img_pil = to_pil_image(pred_img)\n",
    "        img_pil.save(os.path.join(output_dir, output_filename))\n",
    "        \n",
    "    print(f\"saved prediction to: {output_dir}\")\n",
    "\n",
    "def load_custom_fasterrcnn_model(model_path, num_classes):\n",
    "    \"\"\"\n",
    "    Carica un modello Faster R-CNN ResNet50 addestrato per un task personalizzato,\n",
    "    usando i pesi salvati con `state_dict`.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Percorso del file .pth contenente lo state_dict salvato.\n",
    "        num_classes (int): Numero totale di classi (incluso lo sfondo).\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Modello caricato e spostato su GPU (se disponibile).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Carica il modello base\n",
    "    model = get_model(num_classes=num_classes)\n",
    "\n",
    "    # Carica i pesi salvati\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device,weights_only=True))\n",
    "\n",
    "    # Sposta il modello sul dispositivo corretto\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def main(experiment_name):\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    dataset_root = \"dataset\"\n",
    "    save_weights_dir = f\"{experiment_name}/weights\"\n",
    "    num_epochs = 10\n",
    "    writer = SummaryWriter(f\"logs/faster_rcnn50/{experiment_name}\")\n",
    "\n",
    "    transforms = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset_train = VOCDataset(dataset_root, image_set=\"train\", transforms=transforms)\n",
    "    dataset_valid = VOCDataset(dataset_root, image_set=\"val\", transforms=transforms)\n",
    "    dataset_test = VOCDataset(dataset_root, image_set=\"test\", transforms=transforms)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    data_loader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    class_names = dataset_train.classes\n",
    "    model = get_model(num_classes=len(dataset_train.classes))\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    best_score_valid = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_loss_valid = 0.0\n",
    "        print(f\"Epoca {epoch+1}/{num_epochs}: \\n\\tLoss sul train set:\")\n",
    "        for images, targets in tqdm(data_loader_train, desc=f\"\\t\\t\"):\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            total_loss += losses.item()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        print(f\"\\t\\t-Score: {total_loss:.4f}\")\n",
    "        print(f\"\\tLoss sul Valid set:\")\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for images, targets in tqdm(data_loader_valid, desc=f\"\\t\\t\"):\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                \n",
    "                total_loss_valid += losses.item()\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        print(f\"\\t\\t-Score: {total_loss_valid:.4f}\")    \n",
    "        writer.add_scalar(\"loss/train\", total_loss, epoch+1)\n",
    "        writer.add_scalar(\"loss/valid\", total_loss_valid, epoch+1)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        model.eval()  #Entra in modalità evaluation\n",
    "        #EVALUATION PER EPOCA SUL VALID SET\n",
    "        print(\"\\tEvaluation sul valid set:\")\n",
    "        current_score_valid = evaluate_metrics(experiment_name=experiment_name,model=model, data_loader=data_loader_valid, device=device, epoch=epoch+1,set=\"valid\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        os.makedirs(save_weights_dir, exist_ok=True)\n",
    "        \n",
    "        if current_score_valid > best_score_valid:\n",
    "            best_score_valid = current_score_valid\n",
    "            torch.save(model.state_dict(), f\"{experiment_name}/weights/fasterrcnn_voc_best.pth\")\n",
    "                \n",
    "            \n",
    "            #print(f\"New best model saved with score: {best_score:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{experiment_name}/weights/fasterrcnn_voc_last.pth\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    del model\n",
    "    \n",
    "\n",
    "    print(\"Training complete. Model saved.\")\n",
    "    \n",
    "\n",
    "    #EVALUATION SUL TEST SET CON IL BEST MODEL\n",
    "\n",
    "    model =  load_custom_fasterrcnn_model(model_path=f\"{experiment_name}/weights/fasterrcnn_voc_best.pth\", num_classes=len(dataset_train.classes))\n",
    "    print(\"Score sul test set:\\t\")\n",
    "\n",
    "    evaluate_metrics(experiment_name=experiment_name,model=model, data_loader=data_loader_test, device=device, epoch=epoch+1,set=\"test\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    #fase di inferenza\n",
    "\n",
    "    \n",
    "    show_predictions(experiment_name=experiment_name,model=model,dataset=dataset_test,device=device,score_threshold=0.25,class_names=class_names)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"prova5\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#mettere nel file csv le metriche di ogni epoca ✅\n",
    "#inserire weight and biases (oppure tensorboard) ✅\n",
    "#salvare il best.pth del modello ✅\n",
    "#fare la evaluation sul valid set oltre a quella di train(aggiungi la loss/val su tensorboard), salvare il best.pth in base alla map sul valid set(segna epoca nel file) ✅\n",
    "#fare evaluation sul test set con il best.pth salvato ✅\n",
    "#refactor con divisione in celle, oppure vari file.py\n",
    "#dare le etichette delle classi e le probabilità nelle predizioni ✅\n",
    "#cambiare il colore della bounding box in base alla classe in maniera randomica \n",
    "#(opzionale) rendere il codice efficiente(memoria) anche per dispositivi non cuda \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc136765",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
